{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0002\n",
    "training_epoch=100\n",
    "batch_size=100\n",
    "n_hidden = 256\n",
    "n_input=28*28\n",
    "n_noise=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32, [None,n_input])\n",
    "Z=tf.placeholder(tf.float32, [None,n_noise])\n",
    "\n",
    "G_W1=tf.Variable(tf.random_normal([n_noise,n_hidden], stddev=0.01))\n",
    "G_b1=tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2=tf.Variable(tf.random_normal([n_hidden,n_input], stddev=0.01))\n",
    "G_b2=tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "D_W1=tf.Variable(tf.random_normal([n_input,n_hidden], stddev=0.01))\n",
    "D_b1=tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2=tf.Variable(tf.random_normal([n_hidden,1], stddev=0.01))\n",
    "D_b2=tf.Variable(tf.zeros([1]))\n",
    "\n",
    "def generator(noise_z):\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z,G_W1)+G_b1)\n",
    "    output = tf.sigmoid(tf.matmul(hidden,G_W2)+G_b2)\n",
    "    return output\n",
    "\n",
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(tf.matmul(inputs,D_W1)+D_b1)\n",
    "    output = tf.sigmoid(tf.matmul(hidden,D_W2)+D_b2)\n",
    "    return output\n",
    "    \n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size,n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=generator(Z)\n",
    "D_gene = discriminator(G)\n",
    "D_real = discriminator(X)\n",
    "\n",
    "loss_D = tf.reduce_mean(tf.log(1-D_gene)+tf.log(D_real))\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))\n",
    "\n",
    "vars_D=[D_W1,D_b1,D_W2,D_b2]\n",
    "vars_G=[G_W1,G_b1,G_W2,G_b2]\n",
    "\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D,var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G,var_list=vars_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Start\n",
      "Epoch 0001 loss_D:-0.506 loss_G:-2.047\n",
      "Epoch 0002 loss_D:-0.3591 loss_G:-2.189\n",
      "Epoch 0003 loss_D:-0.1842 loss_G:-2.871\n",
      "Epoch 0004 loss_D:-0.373 loss_G:-1.738\n",
      "Epoch 0005 loss_D:-0.4526 loss_G:-1.871\n",
      "Epoch 0006 loss_D:-0.2132 loss_G:-2.583\n",
      "Epoch 0007 loss_D:-0.2553 loss_G:-2.741\n",
      "Epoch 0008 loss_D:-0.375 loss_G:-2.545\n",
      "Epoch 0009 loss_D:-0.28 loss_G:-2.443\n",
      "Epoch 0010 loss_D:-0.609 loss_G:-2.166\n",
      "Epoch 0011 loss_D:-0.3915 loss_G:-2.394\n",
      "Epoch 0012 loss_D:-0.3562 loss_G:-2.288\n",
      "Epoch 0013 loss_D:-0.4521 loss_G:-2.134\n",
      "Epoch 0014 loss_D:-0.6511 loss_G:-1.791\n",
      "Epoch 0015 loss_D:-0.4191 loss_G:-2.285\n",
      "Epoch 0016 loss_D:-0.6126 loss_G:-1.897\n",
      "Epoch 0017 loss_D:-0.609 loss_G:-2.373\n",
      "Epoch 0018 loss_D:-0.7524 loss_G:-1.624\n",
      "Epoch 0019 loss_D:-0.5085 loss_G:-2.107\n",
      "Epoch 0020 loss_D:-0.4471 loss_G:-2.104\n",
      "Epoch 0021 loss_D:-0.4868 loss_G:-2.234\n",
      "Epoch 0022 loss_D:-0.4416 loss_G:-2.275\n",
      "Epoch 0023 loss_D:-0.5944 loss_G:-2.068\n",
      "Epoch 0024 loss_D:-0.5227 loss_G:-2.359\n",
      "Epoch 0025 loss_D:-0.4976 loss_G:-2.25\n",
      "Epoch 0026 loss_D:-0.5704 loss_G:-2.201\n",
      "Epoch 0027 loss_D:-0.5466 loss_G:-2.222\n",
      "Epoch 0028 loss_D:-0.612 loss_G:-2.123\n",
      "Epoch 0029 loss_D:-0.5731 loss_G:-2.187\n",
      "Epoch 0030 loss_D:-0.5124 loss_G:-2.062\n",
      "Epoch 0031 loss_D:-0.4085 loss_G:-2.495\n",
      "Epoch 0032 loss_D:-0.4422 loss_G:-2.395\n",
      "Epoch 0033 loss_D:-0.5988 loss_G:-2.237\n",
      "Epoch 0034 loss_D:-0.4328 loss_G:-2.219\n",
      "Epoch 0035 loss_D:-0.5501 loss_G:-2.29\n",
      "Epoch 0036 loss_D:-0.5006 loss_G:-2.33\n",
      "Epoch 0037 loss_D:-0.6031 loss_G:-1.95\n",
      "Epoch 0038 loss_D:-0.6786 loss_G:-2.31\n",
      "Epoch 0039 loss_D:-0.7246 loss_G:-2.277\n",
      "Epoch 0040 loss_D:-0.5176 loss_G:-2.297\n",
      "Epoch 0041 loss_D:-0.5894 loss_G:-2.657\n",
      "Epoch 0042 loss_D:-0.6114 loss_G:-2.224\n",
      "Epoch 0043 loss_D:-0.5729 loss_G:-2.179\n",
      "Epoch 0044 loss_D:-0.9106 loss_G:-2.208\n",
      "Epoch 0045 loss_D:-0.6657 loss_G:-2.035\n",
      "Epoch 0046 loss_D:-0.7114 loss_G:-1.777\n",
      "Epoch 0047 loss_D:-0.7819 loss_G:-1.873\n",
      "Epoch 0048 loss_D:-0.72 loss_G:-2.042\n",
      "Epoch 0049 loss_D:-0.7162 loss_G:-1.888\n",
      "Epoch 0050 loss_D:-0.7275 loss_G:-2.275\n",
      "Epoch 0051 loss_D:-0.7598 loss_G:-1.772\n",
      "Epoch 0052 loss_D:-0.7611 loss_G:-1.952\n",
      "Epoch 0053 loss_D:-0.8132 loss_G:-1.822\n",
      "Epoch 0054 loss_D:-0.7677 loss_G:-1.871\n",
      "Epoch 0055 loss_D:-0.7158 loss_G:-2.331\n",
      "Epoch 0056 loss_D:-0.8837 loss_G:-1.721\n",
      "Epoch 0057 loss_D:-0.6689 loss_G:-1.886\n",
      "Epoch 0058 loss_D:-0.7817 loss_G:-1.982\n",
      "Epoch 0059 loss_D:-0.6444 loss_G:-1.884\n",
      "Epoch 0060 loss_D:-0.6793 loss_G:-2.159\n",
      "Epoch 0061 loss_D:-0.8005 loss_G:-1.966\n",
      "Epoch 0062 loss_D:-0.7913 loss_G:-1.92\n",
      "Epoch 0063 loss_D:-0.7566 loss_G:-1.876\n",
      "Epoch 0064 loss_D:-0.7107 loss_G:-1.7\n",
      "Epoch 0065 loss_D:-0.7304 loss_G:-2.202\n",
      "Epoch 0066 loss_D:-0.7948 loss_G:-1.668\n",
      "Epoch 0067 loss_D:-0.814 loss_G:-1.909\n",
      "Epoch 0068 loss_D:-0.8625 loss_G:-1.731\n",
      "Epoch 0069 loss_D:-0.7508 loss_G:-1.79\n",
      "Epoch 0070 loss_D:-0.7909 loss_G:-1.909\n",
      "Epoch 0071 loss_D:-0.8228 loss_G:-1.761\n",
      "Epoch 0072 loss_D:-0.7641 loss_G:-1.792\n",
      "Epoch 0073 loss_D:-0.7761 loss_G:-1.674\n",
      "Epoch 0074 loss_D:-0.7511 loss_G:-1.749\n",
      "Epoch 0075 loss_D:-0.9339 loss_G:-1.77\n",
      "Epoch 0076 loss_D:-0.9008 loss_G:-1.794\n",
      "Epoch 0077 loss_D:-0.8078 loss_G:-2.101\n",
      "Epoch 0078 loss_D:-0.7807 loss_G:-1.715\n",
      "Epoch 0079 loss_D:-0.8139 loss_G:-1.935\n",
      "Epoch 0080 loss_D:-0.7066 loss_G:-1.887\n",
      "Epoch 0081 loss_D:-0.9003 loss_G:-1.575\n",
      "Epoch 0082 loss_D:-0.7974 loss_G:-1.792\n",
      "Epoch 0083 loss_D:-0.874 loss_G:-1.8\n",
      "Epoch 0084 loss_D:-0.8399 loss_G:-1.793\n",
      "Epoch 0085 loss_D:-0.8213 loss_G:-1.859\n",
      "Epoch 0086 loss_D:-0.8319 loss_G:-1.66\n",
      "Epoch 0087 loss_D:-0.7473 loss_G:-1.932\n",
      "Epoch 0088 loss_D:-0.8215 loss_G:-1.682\n",
      "Epoch 0089 loss_D:-0.744 loss_G:-1.948\n",
      "Epoch 0090 loss_D:-0.7711 loss_G:-1.946\n",
      "Epoch 0091 loss_D:-0.7309 loss_G:-1.712\n",
      "Epoch 0092 loss_D:-1.018 loss_G:-1.577\n",
      "Epoch 0093 loss_D:-0.7005 loss_G:-1.958\n",
      "Epoch 0094 loss_D:-0.7016 loss_G:-1.974\n",
      "Epoch 0095 loss_D:-0.7835 loss_G:-1.897\n",
      "Epoch 0096 loss_D:-0.7821 loss_G:-1.728\n",
      "Epoch 0097 loss_D:-0.7407 loss_G:-1.818\n",
      "Epoch 0098 loss_D:-0.7523 loss_G:-2.027\n",
      "Epoch 0099 loss_D:-0.7095 loss_G:-1.715\n",
      "Epoch 0100 loss_D:-0.6672 loss_G:-2.035\n",
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch=int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D, loss_val_G =0, 0\n",
    "\n",
    "print('Learning Start')\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size,n_noise)\n",
    "        \n",
    "        _,loss_val_D = sess.run([train_D,loss_D],feed_dict={X:batch_xs, Z:noise})\n",
    "        _,loss_val_G = sess.run([train_G,loss_G],feed_dict={Z:noise})\n",
    "        \n",
    "    print('Epoch','%04d'%(epoch+1), \n",
    "          'loss_D:{:.4}'.format(loss_val_D),\n",
    "          'loss_G:{:.4}'.format(loss_val_G))\n",
    "    if epoch ==0 or (epoch+1)%10 == 0 :\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict = {Z:noise})\n",
    "\n",
    "        fig, ax = plt.subplots(1, sample_size, figsize = (sample_size,1))\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i],(28,28)))\n",
    "        \n",
    "        plt.savefig('./result/{}.png'.format(str(epoch).zfill(3)),\n",
    "                        bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print('Learning Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
