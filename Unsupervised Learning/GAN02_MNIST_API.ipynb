{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0002\n",
    "training_epoch=100\n",
    "batch_size=100\n",
    "n_hidden = 256\n",
    "n_input=28*28\n",
    "n_noise=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32, [None,n_input])\n",
    "Z=tf.placeholder(tf.float32, [None,n_noise])\n",
    "'''\n",
    "G_W1=tf.Variable(tf.random_normal([n_noise,n_hidden], stddev=0.01))\n",
    "G_b1=tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2=tf.Variable(tf.random_normal([n_hidden,n_input], stddev=0.01))\n",
    "G_b2=tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "D_W1=tf.Variable(tf.random_normal([n_input,n_hidden], stddev=0.01))\n",
    "D_b1=tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2=tf.Variable(tf.random_normal([n_hidden,1], stddev=0.01))\n",
    "D_b2=tf.Variable(tf.zeros([1]))\n",
    "'''\n",
    "\n",
    "def generator(noise_z):\n",
    "    with tf.variable_scope('generator'):\n",
    "        hidden = tf.layers.dense(noise_z,n_hidden,activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden,n_input, activation=tf.nn.sigmoid)\n",
    "    return output\n",
    "\n",
    "def discriminator(inputs, reuse=False):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        \n",
    "        hidden = tf.layers.dense(inputs,n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden,1, activation=tf.nn.sigmoid)\n",
    "    return output\n",
    "    \n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size,n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=generator(Z)\n",
    "D_gene = discriminator(G)\n",
    "D_real = discriminator(X,True)\n",
    "\n",
    "loss_D = tf.reduce_mean(tf.log(1-D_gene)+tf.log(D_real))\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))\n",
    "\n",
    "vars_D=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "vars_G=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D,var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G,var_list=vars_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Start\n",
      "Epoch 0001 loss_D:-0.1271 loss_G:-3.326\n",
      "Epoch 0002 loss_D:-0.05919 loss_G:-3.919\n",
      "Epoch 0003 loss_D:-0.05487 loss_G:-4.733\n",
      "Epoch 0004 loss_D:-0.02974 loss_G:-5.463\n",
      "Epoch 0005 loss_D:-0.01891 loss_G:-5.258\n",
      "Epoch 0006 loss_D:-0.09827 loss_G:-3.062\n",
      "Epoch 0007 loss_D:-0.05337 loss_G:-3.785\n",
      "Epoch 0008 loss_D:-0.1593 loss_G:-3.424\n",
      "Epoch 0009 loss_D:-0.1884 loss_G:-3.07\n",
      "Epoch 0010 loss_D:-0.1951 loss_G:-3.303\n",
      "Epoch 0011 loss_D:-0.2267 loss_G:-3.354\n",
      "Epoch 0012 loss_D:-0.1468 loss_G:-3.94\n",
      "Epoch 0013 loss_D:-0.3468 loss_G:-3.329\n",
      "Epoch 0014 loss_D:-0.2965 loss_G:-3.472\n",
      "Epoch 0015 loss_D:-0.1899 loss_G:-3.455\n",
      "Epoch 0016 loss_D:-0.1772 loss_G:-3.638\n",
      "Epoch 0017 loss_D:-0.1522 loss_G:-4.37\n",
      "Epoch 0018 loss_D:-0.2753 loss_G:-3.833\n",
      "Epoch 0019 loss_D:-0.1634 loss_G:-4.258\n",
      "Epoch 0020 loss_D:-0.2034 loss_G:-3.468\n",
      "Epoch 0021 loss_D:-0.2004 loss_G:-3.814\n",
      "Epoch 0022 loss_D:-0.0903 loss_G:-4.713\n",
      "Epoch 0023 loss_D:-0.1491 loss_G:-4.307\n",
      "Epoch 0024 loss_D:-0.3099 loss_G:-4.335\n",
      "Epoch 0025 loss_D:-0.1534 loss_G:-4.566\n",
      "Epoch 0026 loss_D:-0.18 loss_G:-3.912\n",
      "Epoch 0027 loss_D:-0.2057 loss_G:-3.925\n",
      "Epoch 0028 loss_D:-0.186 loss_G:-4.564\n",
      "Epoch 0029 loss_D:-0.2109 loss_G:-4.305\n",
      "Epoch 0030 loss_D:-0.05277 loss_G:-5.465\n",
      "Epoch 0031 loss_D:-0.2466 loss_G:-4.108\n",
      "Epoch 0032 loss_D:-0.1975 loss_G:-4.694\n",
      "Epoch 0033 loss_D:-0.6435 loss_G:-3.07\n",
      "Epoch 0034 loss_D:-0.2554 loss_G:-4.015\n",
      "Epoch 0035 loss_D:-0.1993 loss_G:-4.163\n",
      "Epoch 0036 loss_D:-0.409 loss_G:-3.145\n",
      "Epoch 0037 loss_D:-0.1749 loss_G:-3.882\n",
      "Epoch 0038 loss_D:-0.1776 loss_G:-4.977\n",
      "Epoch 0039 loss_D:-0.2679 loss_G:-3.883\n",
      "Epoch 0040 loss_D:-0.5792 loss_G:-3.718\n",
      "Epoch 0041 loss_D:-0.07598 loss_G:-4.64\n",
      "Epoch 0042 loss_D:-0.1521 loss_G:-4.052\n",
      "Epoch 0043 loss_D:-0.1522 loss_G:-4.091\n",
      "Epoch 0044 loss_D:-0.2155 loss_G:-3.868\n",
      "Epoch 0045 loss_D:-0.239 loss_G:-3.974\n",
      "Epoch 0046 loss_D:-0.4123 loss_G:-3.498\n",
      "Epoch 0047 loss_D:-0.3661 loss_G:-3.597\n",
      "Epoch 0048 loss_D:-0.4214 loss_G:-3.604\n",
      "Epoch 0049 loss_D:-0.2807 loss_G:-3.238\n",
      "Epoch 0050 loss_D:-0.2993 loss_G:-3.322\n",
      "Epoch 0051 loss_D:-0.2845 loss_G:-3.248\n",
      "Epoch 0052 loss_D:-0.3461 loss_G:-3.061\n",
      "Epoch 0053 loss_D:-0.3323 loss_G:-3.339\n",
      "Epoch 0054 loss_D:-0.3139 loss_G:-3.709\n",
      "Epoch 0055 loss_D:-0.3071 loss_G:-3.272\n",
      "Epoch 0056 loss_D:-0.2735 loss_G:-3.275\n",
      "Epoch 0057 loss_D:-0.3203 loss_G:-3.352\n",
      "Epoch 0058 loss_D:-0.3188 loss_G:-3.1\n",
      "Epoch 0059 loss_D:-0.3683 loss_G:-2.988\n",
      "Epoch 0060 loss_D:-0.5777 loss_G:-2.883\n",
      "Epoch 0061 loss_D:-0.3161 loss_G:-3.113\n",
      "Epoch 0062 loss_D:-0.3814 loss_G:-3.035\n",
      "Epoch 0063 loss_D:-0.5393 loss_G:-2.753\n",
      "Epoch 0064 loss_D:-0.5505 loss_G:-2.732\n",
      "Epoch 0065 loss_D:-0.5176 loss_G:-2.933\n",
      "Epoch 0066 loss_D:-0.4565 loss_G:-2.728\n",
      "Epoch 0067 loss_D:-0.4605 loss_G:-2.854\n",
      "Epoch 0068 loss_D:-0.5357 loss_G:-2.936\n",
      "Epoch 0069 loss_D:-0.4616 loss_G:-2.631\n",
      "Epoch 0070 loss_D:-0.6282 loss_G:-2.831\n",
      "Epoch 0071 loss_D:-0.4506 loss_G:-2.875\n",
      "Epoch 0072 loss_D:-0.5272 loss_G:-2.98\n",
      "Epoch 0073 loss_D:-0.4569 loss_G:-3.271\n",
      "Epoch 0074 loss_D:-0.437 loss_G:-2.911\n",
      "Epoch 0075 loss_D:-0.4274 loss_G:-2.94\n",
      "Epoch 0076 loss_D:-0.5385 loss_G:-2.62\n",
      "Epoch 0077 loss_D:-0.4114 loss_G:-2.696\n",
      "Epoch 0078 loss_D:-0.5199 loss_G:-2.718\n",
      "Epoch 0079 loss_D:-0.4747 loss_G:-2.608\n",
      "Epoch 0080 loss_D:-0.6522 loss_G:-2.747\n",
      "Epoch 0081 loss_D:-0.4317 loss_G:-2.838\n",
      "Epoch 0082 loss_D:-0.4979 loss_G:-2.635\n",
      "Epoch 0083 loss_D:-0.5741 loss_G:-2.49\n",
      "Epoch 0084 loss_D:-0.505 loss_G:-2.87\n",
      "Epoch 0085 loss_D:-0.6625 loss_G:-2.394\n",
      "Epoch 0086 loss_D:-0.5953 loss_G:-2.382\n",
      "Epoch 0087 loss_D:-0.5554 loss_G:-2.326\n",
      "Epoch 0088 loss_D:-0.4625 loss_G:-2.616\n",
      "Epoch 0089 loss_D:-0.3911 loss_G:-2.428\n",
      "Epoch 0090 loss_D:-0.556 loss_G:-2.223\n",
      "Epoch 0091 loss_D:-0.5171 loss_G:-2.733\n",
      "Epoch 0092 loss_D:-0.524 loss_G:-2.968\n",
      "Epoch 0093 loss_D:-0.4517 loss_G:-2.55\n",
      "Epoch 0094 loss_D:-0.6056 loss_G:-2.476\n",
      "Epoch 0095 loss_D:-0.486 loss_G:-3.123\n",
      "Epoch 0096 loss_D:-0.5229 loss_G:-2.939\n",
      "Epoch 0097 loss_D:-0.4923 loss_G:-2.365\n",
      "Epoch 0098 loss_D:-0.5056 loss_G:-3.021\n",
      "Epoch 0099 loss_D:-0.5413 loss_G:-2.56\n",
      "Epoch 0100 loss_D:-0.5355 loss_G:-2.55\n",
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch=int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D, loss_val_G =0, 0\n",
    "\n",
    "print('Learning Start')\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size,n_noise)\n",
    "        \n",
    "        _,loss_val_D = sess.run([train_D,loss_D],feed_dict={X:batch_xs, Z:noise})\n",
    "        _,loss_val_G = sess.run([train_G,loss_G],feed_dict={Z:noise})\n",
    "        \n",
    "    print('Epoch','%04d'%(epoch+1), \n",
    "          'loss_D:{:.4}'.format(loss_val_D),\n",
    "          'loss_G:{:.4}'.format(loss_val_G))\n",
    "    if epoch ==0 or (epoch+1)%10 == 0 :\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict = {Z:noise})\n",
    "\n",
    "        fig, ax = plt.subplots(1, sample_size, figsize = (sample_size,1))\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i],(28,28)))\n",
    "        \n",
    "        plt.savefig('./result/{}.png'.format(str(epoch).zfill(3)),\n",
    "                        bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print('Learning Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
