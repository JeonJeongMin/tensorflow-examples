{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-eabbc2171788>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../../mnist/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../../mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../../mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../../../mnist/data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epoch=20\n",
    "batch_size=100\n",
    "n_noise=100\n",
    "n_class=10\n",
    "\n",
    "D_global_step = tf.Variable(0, trainable=False, name='D_global_step')\n",
    "G_global_step = tf.Variable(0, trainable=False, name='G_global_step')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32, [None,28,28,1])\n",
    "Y=tf.placeholder(tf.float32, [None,10])\n",
    "Z=tf.placeholder(tf.float32, [None,n_noise])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    " \n",
    "def leaky_relu(x, leak=0.2):\n",
    "    return tf.maximum(x, x * leak)\n",
    "\n",
    "def generator(noise, labels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        output = tf.concat([noise,labels],1)\n",
    "        output = tf.layers.dense(noise, 128*7*7)\n",
    "        output = tf.reshape(output, [-1, 7, 7, 128])\n",
    "        output = tf.nn.relu(tf.layers.batch_normalization(output, training=is_training))\n",
    "        output = tf.layers.conv2d_transpose(output, 64, [5, 5], strides=(2, 2), padding='SAME')\n",
    "        output = tf.nn.relu(tf.layers.batch_normalization(output, training=is_training))\n",
    "        output = tf.layers.conv2d_transpose(output, 32, [5, 5], strides=(2, 2), padding='SAME')\n",
    "        output = tf.nn.relu(tf.layers.batch_normalization(output, training=is_training))\n",
    "        output = tf.layers.conv2d_transpose(output, 1, [5, 5], strides=(1, 1), padding='SAME')\n",
    "        output = tf.tanh(output)\n",
    "    return output\n",
    "\n",
    "def discriminator(inputs, labels, reuse=False):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        output = tf.layers.conv2d(inputs, 32, [5, 5], strides=(2, 2), padding='SAME')\n",
    "        output = leaky_relu(output)\n",
    "        output = tf.layers.conv2d(output, 64, [5, 5], strides=(2, 2), padding='SAME')\n",
    "        output = leaky_relu(tf.layers.batch_normalization(output, training=is_training))\n",
    "        output = tf.layers.conv2d(output, 128, [5, 5], strides=(2, 2), padding='SAME')\n",
    "        output = leaky_relu(tf.layers.batch_normalization(output, training=is_training))\n",
    "        output = tf.layers.flatten(output)\n",
    "        output = tf.concat([output, labels], 1)\n",
    "        output = tf.layers.dense(output, 1, activation=None)\n",
    "    return output\n",
    "    \n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1.,1.,size=(batch_size,n_noise))\n",
    "\n",
    "def get_moving_noise(batch_size, n_noise):\n",
    "    assert batch_size > 0\n",
    " \n",
    "    noise_list = []\n",
    "    base_noise = np.random.uniform(-1.0, 1.0, size=[n_noise])\n",
    "    end_noise = np.random.uniform(-1.0, 1.0, size=[n_noise])\n",
    " \n",
    "    step = (end_noise - base_noise) / batch_size\n",
    "    noise = np.copy(base_noise)\n",
    "    for _ in range(batch_size - 1):\n",
    "        noise_list.append(noise)\n",
    "        noise = noise + step\n",
    "    noise_list.append(end_noise)\n",
    "    \n",
    "    return noise_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=generator(Z, Y)\n",
    "D_real = discriminator(X, Y)\n",
    "D_gene = discriminator(G, Y, True)\n",
    "\n",
    "loss_D_gene = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
    "loss_D_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=D_real, labels=tf.ones_like(D_real)))\n",
    "                                                      \n",
    "loss_D = loss_D_gene+loss_D_real                                                      \n",
    "loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=D_gene, labels=tf.ones_like(D_gene)))\n",
    "\n",
    "vars_D=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "vars_G=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_D = tf.train.AdamOptimizer().minimize(loss_D,\n",
    "        var_list=vars_D, global_step=D_global_step)\n",
    "    train_G = tf.train.AdamOptimizer().minimize(loss_G,\n",
    "        var_list=vars_G, global_step=G_global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Start\n",
      "Epoch 0001 loss_D:0.02394 loss_G:9.515\n",
      "Epoch 0002 loss_D:0.07885 loss_G:6.699\n",
      "Epoch 0003 loss_D:0.08599 loss_G:9.697\n",
      "Epoch 0004 loss_D:0.04001 loss_G:7.656\n",
      "Epoch 0005 loss_D:0.1099 loss_G:6.739\n",
      "Epoch 0006 loss_D:0.1989 loss_G:5.166\n",
      "Epoch 0007 loss_D:0.3036 loss_G:3.307\n",
      "Epoch 0008 loss_D:0.2942 loss_G:3.799\n",
      "Epoch 0009 loss_D:0.2847 loss_G:3.098\n",
      "Epoch 0010 loss_D:0.6763 loss_G:1.7\n",
      "Epoch 0011 loss_D:1.292 loss_G:2.073\n",
      "Epoch 0012 loss_D:0.2728 loss_G:3.617\n",
      "Epoch 0013 loss_D:0.1885 loss_G:3.765\n",
      "Epoch 0014 loss_D:0.2002 loss_G:3.888\n",
      "Epoch 0015 loss_D:1.091 loss_G:1.669\n",
      "Epoch 0016 loss_D:1.475 loss_G:3.326\n",
      "Epoch 0017 loss_D:0.348 loss_G:2.779\n",
      "Epoch 0018 loss_D:0.641 loss_G:2.583\n",
      "Epoch 0019 loss_D:0.3252 loss_G:2.452\n",
      "Epoch 0020 loss_D:0.48 loss_G:2.899\n",
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    loss_val_D, loss_val_G =0, 0\n",
    "\n",
    "    print('Learning Start')\n",
    "\n",
    "    for epoch in range(training_epoch):\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "            noise = get_noise(batch_size,n_noise)\n",
    "\n",
    "            _,loss_val_D = sess.run([train_D,loss_D],\n",
    "                                    feed_dict={X:batch_xs, Z:noise, Y:batch_ys, is_training:True})\n",
    "            _,loss_val_G = sess.run([train_G,loss_G],\n",
    "                                    feed_dict={X:batch_xs, Z:noise, Y:batch_ys, is_training:True})\n",
    "\n",
    "        print('Epoch','%04d'%(epoch+1), \n",
    "              'loss_D:{:.4}'.format(loss_val_D),\n",
    "              'loss_G:{:.4}'.format(loss_val_G))\n",
    "        if epoch != -1 :\n",
    "            sample_size = 10\n",
    "            noise = get_noise(sample_size, n_noise)\n",
    "            samples = sess.run(G, \n",
    "                               feed_dict = {Z:noise, is_training:False})\n",
    "            test_noise = get_moving_noise(sample_size, n_noise)\n",
    "            test_samples = sess.run(G, feed_dict={Y:mnist.test.labels[:sample_size], Z: test_noise, is_training: False})\n",
    "\n",
    "            fig, ax = plt.subplots(2, sample_size, figsize = (sample_size,2))\n",
    "\n",
    "            for i in range(sample_size):\n",
    "                ax[0][i].set_axis_off()\n",
    "                ax[1][i].set_axis_off()\n",
    "                ax[0][i].imshow(np.reshape(samples[i],(28,28)))\n",
    "                ax[1][i].imshow(np.reshape(test_samples[i],(28,28)))\n",
    "\n",
    "            plt.savefig('./result2_conditional/{}.png'.format(str(epoch).zfill(3)),\n",
    "                            bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "    print('Learning Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f573d1cb7505>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0msample_show\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-f573d1cb7505>\u001b[0m in \u001b[0;36msample_show\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msample_show\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1073\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1075\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1076\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot(i):\n",
    "    return sess.run(tf.one_hot([i],10))\n",
    "\n",
    "def sample_show(i):\n",
    "    a=sess.run(tf.one_hot([i],10))\n",
    "    for k in range(9):\n",
    "        a=np.append(a,one_hot(i),axis=0)\n",
    "        \n",
    "    sample = sess.run(G,feed_dict={Y:a,Z:get_noise(10,128)})\n",
    "    fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
    "    for k in range(10):\n",
    "            ax[k].set_axis_off()\n",
    "            ax[k].imshow(np.reshape(sample[k],(28,28)))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "sample_show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
